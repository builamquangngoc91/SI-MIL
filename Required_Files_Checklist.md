# Required Files Checklist for PathExpert_feature_extraction.sh

## âœ… **Current Status - You Can Run The Script**

Your setup is **COMPLETE** and ready to run! You have all required files:

### Files You Already Have:

- âœ… **WSI Files**: `data_sample/slides/TCGA-A6-2686-01Z-00-DX1.0540a027-2a0c-46c7-9af0-7b8672631de7.svs`
- âœ… **HoVer-Net Output**: `data_sample/Hovernet_output/json/[WSI_NAME].json`
- âœ… **Train/Test Split**: `data_sample/train_test_dict.json`
- âœ… **Column Name Files**: All pickle files present
- âœ… **Python Scripts**: All extraction scripts present
- âœ… **Previous Results**: Feature extraction already completed

## ğŸ“‹ **Complete Requirements List**

### **1. Input Data Files**

#### WSI Files (Required)

```
data_sample/slides/
â””â”€â”€ [WSI_NAME].svs    # Whole Slide Image files
```

**Format**: `.svs`, `.tiff`, or other OpenSlide-compatible formats  
**Magnification**: Must have 40X magnification available  
**Status**: âœ… You have 1 WSI file

#### HoVer-Net Segmentation Results (Required)

```
data_sample/Hovernet_output/
â”œâ”€â”€ json/
â”‚   â””â”€â”€ [WSI_NAME].json    # Cell segmentation and classification
â””â”€â”€ mask/ (optional)
    â””â”€â”€ [WSI_NAME].png     # Visualization masks
```

**Content**: JSON files containing cell coordinates, types, and properties  
**Generated by**: HoVer-Net model with PanNuke checkpoint  
**Status**: âœ… You have the required JSON files

#### Train/Test Split Definition (Required)

```
data_sample/train_test_dict.json
```

**Format**:

```json
{
  "train_dict": {
    "slide1.svs": 0, // 0=negative, 1=positive
    "slide2.svs": 1
  },
  "test_dict": {
    "slide3.svs": 0,
    "slide4.svs": 1
  }
}
```

**Status**: âœ… You have this file

#### Feature Column Name Files (Required)

```
data_sample/
â”œâ”€â”€ athena_column_name.pickle      # ATHENA feature names
â”œâ”€â”€ cell_stat_column_name.pickle   # Cell statistics feature names
â””â”€â”€ sna_column_name.pickle         # Social network feature names
```

**Content**: Lists of feature column names for proper indexing  
**Status**: âœ… All files present

### **2. Processing Scripts (Required)**

All scripts must be in `data_curation/` directory:

- âœ… `extract_properties.py` - Cell morphology extraction
- âœ… `deepzoom_tiler_organ.py` - Patch extraction
- âœ… `patch_dict_list.py` - Patch indexing
- âœ… `extract_cell_statistics_features.py` - Cell statistics
- âœ… `extract_sna_features.py` - Social network analysis
- âœ… `extract_athena_spatial_features.py` - Spatial heterogeneity
- âœ… `extract_tissue_features.py` - Tissue features
- âœ… `club_features.py` - Feature combination
- âœ… `data_filtering.py` - Final processing

### **3. Directory Structure (Auto-created)**

The script will create these directories if they don't exist:

```
data_sample/
â”œâ”€â”€ cell_property/          # Created by step 1
â”œâ”€â”€ patches/               # Created by step 2
â””â”€â”€ features/
    â”œâ”€â”€ cell_statistics/   # Created by step 4
    â”œâ”€â”€ sna_statistics/    # Created by step 5
    â”œâ”€â”€ athena_statistics/ # Created by step 6
    â””â”€â”€ tissue_statistics/ # Created by step 7
```

## ğŸ”§ **If Starting From Scratch**

### **Step 1: Prepare WSI Files**

1. Place all `.svs` files in `data_sample/slides/`
2. Ensure slides have 40X magnification available
3. Remove any slides without 40X magnification

### **Step 2: Run HoVer-Net Cell Segmentation**

Before running `PathExpert_feature_extraction.sh`, you need HoVer-Net results:

```bash
# Install HoVer-Net
git clone https://github.com/vqdang/hover_net.git
cd hover_net
conda env create -f environment.yml
conda activate hover_net

# Download PanNuke checkpoint
gdown --id 1SbSArI3KOOWHxRlxnjchO7_MbWzB4lNR

# Run HoVer-Net inference
parent_dir='/Users/nam.le/Desktop/research/SI-MIL/data_curation/data_sample'

python run_infer.py \
--gpu='0' \
--nr_types=6 \
--type_info_path=type_info.json \
--batch_size=100 \
--model_mode=fast \
--model_path=hovernet_fast_pannuke_type_tf2pytorch.tar \
--nr_inference_workers=6 \
--nr_post_proc_workers=6 \
wsi \
--input_dir="$parent_dir/slides" \
--proc_mag=40 \
--cache_path='cache' \
--output_dir="$parent_dir/Hovernet_output" \
--input_mask_dir="$parent_dir/Hovernet_output/msk" \
--chunk_shape=10000 \
--save_mask
```

### **Step 3: Create Train/Test Split**

Create `data_sample/train_test_dict.json`:

```json
{
  "train_dict": {
    "slide1.svs": 0,
    "slide2.svs": 1,
    "slide3.svs": 0
  },
  "test_dict": {
    "slide4.svs": 1,
    "slide5.svs": 0
  }
}
```

### **Step 4: Ensure Column Name Files**

Copy from `test_dataset/` or existing working directory:

- `athena_column_name.pickle`
- `cell_stat_column_name.pickle`
- `sna_column_name.pickle`

## ğŸš€ **Ready to Run**

Once you have all the above files, simply execute:

```bash
cd /Users/nam.le/Desktop/research/SI-MIL/data_curation
conda activate simil
chmod +x PathExpert_feature_extraction.sh
./PathExpert_feature_extraction.sh
```

## âš ï¸ **Important Notes**

1. **Path Configuration**: Update the `parent_dir` path in `PathExpert_feature_extraction.sh` if needed
2. **Computational Resources**: Script uses 10 workers per step - adjust based on your system
3. **Storage Space**: Ensure sufficient disk space for patches and feature files
4. **Memory Requirements**: Large WSIs require substantial RAM (8GB+ recommended)
5. **Processing Time**: Can take several hours depending on WSI size and number

## ğŸ” **Verification**

Your setup is ready when you have:

- [ ] WSI files in `slides/`
- [ ] HoVer-Net JSON files in `Hovernet_output/json/`
- [ ] Train/test split JSON file
- [ ] Column name pickle files
- [ ] All Python scripts present

**Your Current Status: âœ… ALL REQUIREMENTS MET**
